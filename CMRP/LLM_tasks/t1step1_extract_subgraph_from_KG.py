import json
import ast
import requests
import os
import re
import random
import torch
from torch import nn
import numpy as np
import csv
import pickle
from transformers import T5Tokenizer, T5EncoderModel
from itertools import permutations, product
from tqdm import tqdm
from collections import defaultdict
import cupy as cp
from concurrent.futures import ProcessPoolExecutor, as_completed
from concurrent.futures import ThreadPoolExecutor

def save_matrix(matrix, filename):
    with open(filename, 'wb') as file:
        pickle.dump(matrix, file)

def load_matrix(filename):
    with open(filename, 'rb') as file:
        return pickle.load(file)

def read_KG_triplets(dataset
                     ):
    train_data_file=f'../knowledge_graph_tasks/embedding_based/benchmarks/{dataset}/org_train2id.txt'
    dev_data_file=f'../knowledge_graph_tasks/embedding_based/benchmarks/{dataset}/valid2id.txt'
    test_data_file=f'../knowledge_graph_tasks/embedding_based/benchmarks/{dataset}/test2id.txt'
    KG_entity_pair = []
    all_KG_triples = []
    entities2rel = {}
    for data_file in [train_data_file, dev_data_file, test_data_file]:
        with open(data_file, 'r') as f:
            file = csv.reader(f, delimiter=' ')
            next(file)
            for line in tqdm(file):
                h, t, r = line
                all_KG_triples.append([h, t, r])

                KG_entity_pair.append((h, t))
                entities2rel[(int(h),int(t))] = r
                entities2rel[(int(t),int(h))] = r

    with open(f'../knowledge_graph_tasks/embedding_based/benchmarks/{dataset}/entity2id.txt', 'r') as f:
        file = csv.reader(f, delimiter='\t')
        num_of_entities = int(next(file)[0])

    KG_entities = list(range(num_of_entities))

    entity2text = {}
    with open(f'../knowledge_graph_tasks/embedding_based/benchmarks/{dataset}/text2entityid.txt', 'r') as f:
        file = csv.reader(f, delimiter='\t')
        for line in file:
            text, entityid = line
            entity2text[entityid] = text

    relation2text = {}
    with open(f'../knowledge_graph_tasks/embedding_based/benchmarks/{dataset}/text2relationid.txt', 'r') as f:
        file = csv.reader(f, delimiter='\t')
        for line in file:
            text, relationid = line
            relation2text[relationid] = text

    return entities2rel, entity2text, relation2text, all_KG_triples, KG_entities, KG_entity_pair


def extract_hg_entities_and_relations():
    # {level: [entity, eid, entityid2embeds[eid]}, entityid2embeds[eid] is generated by T5-small
    hg_entities = pickle.load(open(f'../preprocess/{dataset}_hg_entities.pkl', 'rb'))
    # {level: [relation, rid, relationid2embeds[eid]}, relationid2embeds[eid] is generated by T5-small
    hg_relations = pickle.load(open(f'../preprocess/{dataset}_hg_relations.pkl', 'rb'))
    # print(hg_entities)
    print(len(hg_entities)) #1000
    print(len(hg_relations)) #1000
    entities, relations = defaultdict(list), defaultdict(list)
    for qid, q_list in hg_entities.items():
        if q_list:  # 如果q_list不为空
            for level, entity_info in q_list.items():
                for info in entity_info:
                    entities[qid].append(info[1])
        else:
            print('qid = ',qid,'empty')
            entities[qid] = []  # 对于空的q_list，添加一个空列表

    for qid, q_list in hg_relations.items():
        if q_list:  # 如果q_list不为空
            for level, relation_info in q_list.items():
                for info in relation_info:
                    relations[qid].append(info[1])
        else:
            relations[qid] = []  # 对于空的q_list，添加一个空列表
    #entities和relations分别是说 每个question（对应一个qid）里面包含的entities和relations，对应的id的mapping
    #例如qid=0的时候，可能有entities entities[0] = {123,456,789}
    print(len(entities))
    print(len(relations))
    return entities, relations

note = '''
在for循环中，代码执行以下操作：
遍历hg_entities字典中的每个键值对，其中qid是键，而q_list是对应的值。
对于每个qid，再遍历其对应的q_list字典。这里的level是键，entity_info是值。
entity_info是一个列表，列表中的每个元素都包含一个实体的名称、ID和嵌入。
for info in entity_info:循环遍历entity_info中的每个元素（即每个实体相关的信息）。
entities[qid].append(info[1])：这一步是在为entities字典中的对应qid添加实体的ID。info[1]表示从entity_info中提取的每个实体的ID。
'''

def merge_subgraphs(subgraph_tuples: [[tuple]]) -> dict:

    def dfs_step2(subgraph, grid, i, j):
        if not 0 <= i < len(grid) or not 0 <= j < len(grid[0]) or grid[i][j] == '0': return
        grid[i][j] = '0'
        subgraph.append((i, j))
        dfs_step2(subgraph, grid, i + 1, j)
        dfs_step2(subgraph, grid, i, j + 1)
        dfs_step2(subgraph, grid, i - 1, j)
        dfs_step2(subgraph, grid, i, j - 1)

    all_subgraphs = {}
    for qid, tuples in subgraph_tuples.items():
        print('qid',qid,'merge tuples = ',tuples)
        grid = [['0' for _ in range(len(KG_entities))] for _ in range(len(KG_entities))]
        for (i, j) in tuples:
            grid[i][j] = '1'

        subgraphs = []
        for i in range(len(grid)):
            for j in range(len(grid[0])):
                subgraph = []
                dfs_step2(subgraph, grid, i, j)
                if subgraph != []:
                    subgraphs.append(subgraph)
        all_subgraphs[qid] = subgraphs
        print('qid = ',qid,'subgraphs = ',subgraphs)
    
    return all_subgraphs
def process_single_qid(qid, tuples, KG_entities):
    def dfs_step2(subgraph, grid, i, j):
        if not 0 <= i < len(grid) or not 0 <= j < len(grid[0]) or grid[i][j] == '0': return
        grid[i][j] = '0'
        subgraph.append((i, j))
        dfs_step2(subgraph, grid, i + 1, j)
        dfs_step2(subgraph, grid, i, j + 1)
        dfs_step2(subgraph, grid, i - 1, j)
        dfs_step2(subgraph, grid, i, j - 1)

    print('Processing qid:', qid)
    grid = [['0' for _ in range(len(KG_entities))] for _ in range(len(KG_entities))]
    for (i, j) in tuples:
        grid[i][j] = '1'

    subgraphs = []
    for i in range(len(grid)):
        for j in range(len(grid[0])):
            subgraph = []
            dfs_step2(subgraph, grid, i, j)
            if subgraph:
                subgraphs.append(subgraph)
    return qid, subgraphs

# def merge_subgraphs_parallel(subgraph_tuples, KG_entities, num_threads):
#     all_subgraphs = {}
#     with ThreadPoolExecutor(max_workers=num_threads) as executor:
#         # 创建并行任务
#         future_to_qid = {executor.submit(process_single_qid, qid, tuples, KG_entities): qid for qid, tuples in subgraph_tuples.items()}

#         # 收集结果
#         for future in concurrent.futures.as_completed(future_to_qid):
#             qid = future_to_qid[future]
#             try:
#                 _, subgraphs_for_single_qid = future.result()
#                 all_subgraphs[qid] = subgraphs_for_single_qid
#             except Exception as exc:
#                 print('QID %r generated an exception: %s' % (qid, exc))

#     return all_subgraphs

def merge_subgraphs_parallel(subgraph_tuples, KG_entities, num_threads):
    all_subgraphs = {}
    with ProcessPoolExecutor(max_workers=num_threads) as executor:
        # 创建并行任务
        future_to_qid = {executor.submit(process_single_qid, qid, tuples, KG_entities): qid for qid, tuples in subgraph_tuples.items()}

        # 收集结果
        for future in as_completed(future_to_qid):
            qid = future_to_qid[future]
            try:
                _, subgraphs_for_single_qid = future.result()
                all_subgraphs[qid] = subgraphs_for_single_qid
            except Exception as exc:
                print('QID %r generated an exception: %s' % (qid, exc))

    return all_subgraphs

def matrix_power_gpu(KG_matrix, K):
    # 将矩阵转移到GPU
    KG_matrix_gpu = cp.array(KG_matrix)

    # 计算矩阵的K次幂
    Khop_matrix_gpu = cp.linalg.matrix_power(KG_matrix_gpu, K)

    # 将结果从GPU转移到CPU，并转换回NumPy数组
    Khop_matrix = cp.asnumpy(Khop_matrix_gpu)
    
    return Khop_matrix

def extract_subgraphs_from_log(file_path):
    all_subgraphs = {}

    with open(file_path, 'r') as file:
        lines = file.readlines()

    for i, line in enumerate(lines):
        if "qid" in line and "subgraphs =" in line:
            # 提取qid
            qid_match = re.search(r"qid =  (\d+)", line)
            if qid_match:
                qid = int(qid_match.group(1))

                # 提取并解析subgraphs
                subgraphs_match = re.search(r"subgraphs =  (\[.*\])", line)
                if subgraphs_match:
                    subgraphs_str = subgraphs_match.group(1)
                    subgraphs = ast.literal_eval(subgraphs_str)
                    all_subgraphs[qid] = subgraphs

    return all_subgraphs

def extract_subgraphs_from_KG(entities, KG_entities, KG_entity_pair, entities2rel, entity2text, relation2text):

    # convert KG into matrix
    KG_matrix = [['0' for _ in range(len(KG_entities))] for _ in range(len(KG_entities))]
    for (ent1, ent2) in KG_entity_pair:
        # print('ent1,ent2:',ent1,ent2)
        ent1, ent2 = int(ent1), int(ent2)
        KG_matrix[ent1][ent2] = '1'
        KG_matrix[ent2][ent1] = '1'
            #构建实体矩阵
    # 示例：使用KG_matrix_np（转换为NumPy数组的邻接矩阵）
    subgraph_tuples = defaultdict(list)
    print('not done,Khop_matrix')
    KG_matrix = np.array(KG_matrix, dtype=int)
    Khop_matrix = matrix_power_gpu(KG_matrix, 1)
    save_matrix(Khop_matrix,'Khop_matrix.pkl') #save as pickle file 
    print('done,Khop_matrix')
    # 假设entities是一个字典，映射query ID到实体列表
    for qid, entity_list in entities.items():
        unique_entity_list = list(dict.fromkeys(entity_list))
        print(qid, ',unique_entity_list:', unique_entity_list)
        for entity in unique_entity_list:
            # 遍历K-hop邻接矩阵中的每一行
            for i, value in enumerate(Khop_matrix[int(entity)]):
                if value > 0 and i != int(entity) :
                    # 为每个与entity在K-hop内相连的实体创建元组
                    subgraph_tuples[qid].append((int(entity), i))
        # print('len(subgraph_tuples[qid]) = ',len(subgraph_tuples[qid]))
        # print(subgraph_tuples[qid][0])

    print('K-hop subgraphs extracted for every qid!')
    print('merge_subgraphs...')
    # merge subgraphs: e.g., [[(1, 2), (3, 4)], [(5, 6), (7, 8)], ......]

    # index_based_all_subgraphs = merge_subgraphs(subgraph_tuples)
    # 使用函数
    num_threads = 24  # 根据您的系统资源调整线程数
    # index_based_all_subgraphs = merge_subgraphs_parallel(subgraph_tuples, KG_entities, num_threads)
    index_based_all_subgraphs = merge_subgraphs_parallel(subgraph_tuples, KG_entities, num_threads)

    # print('本次从文件中读取subgraphs')
    # log_file_path = 'runstep1.log'  # 更改为您的日志文件路径
    # # 使用函数提取日志文件中的数据
    # index_based_all_subgraphs = extract_subgraphs_from_log(log_file_path)
    # print('len(index_based_all_subgraphs) = ', len(index_based_all_subgraphs))
    # print('已完成：本次从文件中读取subgraphs')
    text_based_all_subgraphs = {}
    for qid, subgraphs in index_based_all_subgraphs.items():
        subgraphs = [[(entity2text[str(tup[0])], relation2text[str(entities2rel[tup])], entity2text[str(tup[1])]) if tup[0] != tup[1]
                  else (entity2text[str(tup[0])], 'self', entity2text[str(tup[1])]) for tup in subgraph]
                 for subgraph in subgraphs]
        text_based_all_subgraphs[qid] = subgraphs
        print('subgraphs for qid = ',qid,' are converted to text!',subgraphs)

    return text_based_all_subgraphs


dataset = 'YAGO3-10'
entities2rel, entity2text, relation2text, all_KG_triplets, KG_entities, KG_entity_pair = read_KG_triplets(dataset)
entities, relations = extract_hg_entities_and_relations()
#entities和relations分别是说 每个question（对应一个qid）里面包含的entities和relations，对应的id的mapping
#例如qid=0的时候，可能有entities[0] = {123,456,789}
print('hg_entities_and_relations extracted!')
# print('len(entities) = ',len(entities)) #1000
# print('len(relations) = ',len(relations)) #1000
text_based_all_subgraphs = extract_subgraphs_from_KG(entities, KG_entities, KG_entity_pair, entities2rel, entity2text, relation2text)
print('subgraphs extracted!')
pickle.dump(text_based_all_subgraphs, open(f'{dataset}_all_subgraphs_dict.pkl', 'wb'))